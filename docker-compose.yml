version: '3.8'

services:
  api-service:
    build:
      context: .
      dockerfile: docker/api/Dockerfile
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    ports:
      - "8000:8000"
    environment:
      - LLM_SERVICE_URL=http://llm-service:8000
      - MEMORY_DB_PATH=/app/data/memory.db
      - VECTOR_DB_PATH=/app/data/vectordb
    depends_on:
      - llm-service

  llm-service:
    build:
      context: .
      dockerfile: docker/llm/Dockerfile
    volumes:
      - ./data:/app/data
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/app/models # Mount the local models directory
    ports:
      - "8001:8000"
    environment:
      - MODEL_ID=/app/models/meta-llama/Llama-3.2-3B-Instruct # Path inside container
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

  ui-service:
    build:
      context: .
      dockerfile: docker/ui/Dockerfile
    ports:
      - "7860:7860"
    environment:
      - API_URL=http://api-service:8000
    depends_on:
      - api-service
